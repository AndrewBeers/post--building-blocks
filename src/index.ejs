<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>

  <style>
    figure.full-width {
      grid-column: screen;
      margin-left: 100px;
    }

    figure.act-grids {
      overflow: hidden;
    }

    .attribution_list {
      display: inline-block;
      list-style-type: none;
      padding: 0;
      margin: 0;
    }

    .attribution_list li {
      position: relative;
      display: flex;
      margin-bottom: 0;
      font-size: 90%;
      text-align: right;
      text-transform: capitalize;
    }

    .attribution_list li span {
      display: inline-block;
      width: 150px;
      overflow: hidden;
      white-space: nowrap;
      text-overflow: ellipsis;
      margin-right: 10px;
    }

    .attribution_list .scent {
      display: flex;
      position: relative;
      width: 40px;
      align-items: center;
    }

    .attribution_list .scent div {
      position: absolute;
      left: 0;
      background: #ccc;
      height: calc(100% - 10px);
    }
  </style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Article Title",
  "description": "Description of the post",
  "authors": [
    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain",
      "affiliationURL": "https://g.co/brain"
    },
    {
      "author": "Arvind Satyanarayan",
      "authorURL": "http://arvindsatya.com",
      "affiliation": "Google Brain",
      "affiliationURL": "https://g.co/brain"
    },
    {
      "author": "Ian Johnson",
      "authorURL": "https://github.com/enjalot",
      "affiliation": "Google Cloud",
      "affiliationURL": "http://cloud.google.com/"
    },
    {
      "author": "Ludwig Schubert",
      "authorURL": "https://schubert.io/",
      "affiliation": "Google Brain",
      "affiliationURL": "https://g.co/brain"
    },
    {
      "author": "Katherine Ye",
      "authorURL": "https://cs.cmu.edu/~kqy/",
      "affiliation": "CMU",
      "affiliationURL": "https://cs.cmu.edu/"
    },
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    }
  ]
  }</script>
</d-front-matter>

<d-title>
  <!-- <h1>Building Blocks<br />for Interpretability</h1> -->
  <h1>Interpretability as<br />Interface Design</h1>
</d-title>

<d-article>
  <p><b><i>Author list and order not finalized.</i></b></p>

  <p>
    Designing meaningful user interfaces consists of two steps: constructing deep abstractions, and then reifying them <d-cite key="nielsen2016thought"></d-cite>. 
    With a few exceptions <d-cite key="olah2015visualizing,yosinski2015understanding,carter2017using"></d-cite>, existing work on interpretability has primarily focused on the former&mdash;we have seen the development of powerful abstractions, including feature visualization <d-cite key="erhan2009visualizing,olah2017feature,simonyan2013deep,nguyen2015deep,mordvintsev2015inceptionism,nguyen2016plug"></d-cite>, attribution <d-cite key="simonyan2013deep,zeiler2014visualizing,springenberg2014striving,selvaraju2016grad,fong2017interpretable,kindermans2017patternnet,kindermans2017reliability"></d-cite>, and dimensionality reduction <d-cite key="maaten2008visualizing"></d-cite>. 
    However, it appears to us that the corresponding work of reifying these abstractions has been neglected. 
    This neglect has left us with impoverished interfaces (e.g., saliency maps) that leave a lot of value on the table.
    Worse, these interfaces hinder the development of our abstractions by not pushing them to their limits.
  </p>

  <p>
    When we treat interpretability as a user interface design problem, existing techniques become fundamental and composable building blocks.

    We present interfaces that show
    <i>what</i> the network detects and explain
    <i>how</i> it develops its understanding, while keeping things
    <i>human-scale</i>.
    
    For example, we will see how a network looking at a labrador retriever detects floppy ears and how that influences its classification.

    Similarly, when the network looks at an image of a woman with a punching bag, we can observe the model's bias in action:
    the model recognizes a feminine body which decreases the probability of a "punching bag" classification and increase "pole dancing."
    </p>

  <!-- <p>
    In this article, we consider interpretability through the lens of user interface design: iterating between reifying abstractions and pushing them further. 
    This process has allowed us to identify relatively basic operations with a lot of potential.
    For instance, feature visualization naturally combines with a network's activations to create a "semantic dictionary" that makes hidden activations meaningful. 
    Moreover, we find that existing techniques can be applied in more general ways. 
    Attribution, for example, can be layered over semantic dictionaries to understand how concepts evolve through the network's hidden layers. 
  </p>

  <p>
    With this lens, existing techniques now become fundamental and composable building blocks for a new generation of more meaningful user interfaces for interpretability. 
    We present interfaces that show <i>what</i> the network detects and explain <i>how</i> it develops its understanding, while keeping things <i>human-scale</i>.
    For example, we will see how a network looking at a labrador retriever detects floppy ears and how that influences its classification. 
    Similarly, when the network looks at an image of a woman with a punching bag, we can observe the model's bias in action: the model recognizes a feminine body which decreases the probability of a "punching bag" classification and increase "pole dancing."
  </p> -->

  <p>
    From calculus to programming languages, there is a long history of new interface abstractions enabling progress in science and engineering.

    Deep learning increasingly seems like a new computing paradigm, distinct from traditional enumerative algorithms.

    If this is true, we should expect to need corresponding new abstractions.

    We believe the right abstractions will help build models that are fair, robust, and aligned.
  </p>

  <h2>Making Sense of Hidden Layers</h2>

  <p>
    Much of the recent work on interpretability is concerned with a neural network's input and output layers. 
    Arguably, this focus is due to the clear meaning these layers have: in computer vision, the input layer represents values for the red, green, and blue color channels for every pixel in the input image, while the output layer consists of class labels and their associated probabilities.
  </p>

  <p>
    However, the power of neural networks lie in their hidden layers&mdash;at every layer, the network discovers a new representation of the input. 
    In computer vision, we use neural networks that run the same feature detectors at every position in the image. 
    We can think of each layer's learned representation as a three-dimensional cube. Each cell in the cube is an <em>activation</em>, or the amount a neuron fires. 
    The x- and y-axes correspond to positions in the image, and the z-axis is the channel (or detector) being run.
  </p>

  <figure id="ActivationCube" class="l-body"></figure>

  <figure id="ExamplePicker" class="l-body" style="margin: 0px; z-index: 1;"></figure>

  <figure id="SemanticDict"></figure>

  <p>
    This marriage of activations and feature visualizations <i>reifies</i> widespread intuition for what activation vectors really are.
    It's always been suspected -- and increasingly shown -- that neurons are detecting important natural concepts.
    Semantic dictionaries take this from a hazy intuition to an actual way of interacting with neuron activations.
    Reifying ideas like this is a deep kind of user interface work,
    causing the interface to embody a powerful way of thinking about or interacting with an idea <d-cite key="nielsen2016thought"></d-cite>.
  </p>

  <p>
    Beyond this, semantic dictionaries serve as a lingua franca for interpretability.
    By definition, semantic dictionaries combine feature visualization and activations. 
    Just like the underlying vector, we can apply dimensionality reduction to them.
    And, just like any input to a neural network, we can perform attribution. 
    In principle, these techniques could be combined without semantic dictionaries.
    But the semantic dictionary framing naturally makes them into <em>composable</em> building blocks.
  </p>


  <!-- <p>
    This kind of reification is a change in our relationship to a mathematical object.  
    While some work has explored UIs for interpretability techniques, it looks at these techniques in isolation.
    We believe this is a mistake because interpretability is about establishing interfaces.
    
    Interpretability is seeks to create a way for humans to understand neural networks. There are two parts to this: we need to build the fundamental abstractions that allow humans to reason about these systems, and interfaces for us to be able to interact with the abstractions. In a vacuum, the abstracts are useless. All interpretability work, thus far, has created very primitive interfaces (e.g., saliency maps) but they only put a tiny amount of energy into it. We think this is an extremely neglected area of research. By not going and exploring the interfaces to the abstractions, we are leaving a lot of potential on the table, and we are missing out on insights into our abstractions.
    
    The power of abstractions are in the operations you can perform onto them. But until you work with the abstractions,

    These abstractions are typically extremely painful to work with. Or, you can work with them in an extremely limited way that misses out on a lot of their value.

    When interfaces make it easy to push abstractions to their limits, new questions become easier to ask. 
  </p> -->

  <h2>What Does the Network See?</h2>
  
  <figure id="ActivationVecVis" class="l-body" style="margin-top: 0;"></figure>

  <p>
    Applying this technique to all the activation vectors allows us to not only see what the network detects at each position, but also what the network understands of the input image as a whole.
    And, by working across layers, we can observe how the network's understanding evolves: from detecting edges in earlier layers, to more sophisticated shapes and object parts in the latter.
  </p>

  <!-- Include a blown up version of most interesting grid? -->

  <figure id="AllActivationGrids" class="act-grids full-width"></figure>

  <p>
    These visualizations, however, omit a crucial piece of information: the magnitude of the activations. 
    By sizing each of the cells of the visualization by the magnitude of the activation vector, we can indicate how strongly the network detected features at that position:
  </p>

  <figure id="AllActivationGridsMagnitude" class="act-grids full-width"></figure>

  <p>
    Traditional dimensionality reduction techniques, such as principal component analysis, give us another way to understand what the network is detecting. 
    For instance, to get an "overview" of the activation vectors, we can compute the first three principal components of the activations and color the image based on them. 
  </p>

  <p>
    These principle components are also directions in activation space and, thus, have meaning by themselves. 
    Using feature visualization, we can inspect what each of the components represents.
  </p>

  <figure class="l-body">
    <img src="images/activation-pca.png"></img>
  </figure>

  <p>
    In the end, a network's activations can be presented as semantic dictionaries with different bases. 
    There are three particularly natural ones to consider: what an activation vector represents as a whole, the vector in terms of some dimensionality reduction, and finally the individual neurons themselves.
    Note, these all (approximately) describe the same object!
  </p>

  <figure class="l-page">
    <img src="images/vector-eq-full.png"  style="width:100%"></img>
  </figure>

  <h2>How Are Concepts Assembled?</h2>

  <p>
    Feature visualization and dimensionality reduction techniques help answer the first of our three original questions: <em>what</em> does the network detect. However, neither technique allows us to reason about <em>how</em> the network assembled these individual pieces to arrive at later decisions, or <em>why</em> these decisions were made.
  </p>

  <p>
    Attribution is a set of techniques that answers such questions by explaining the relationships between neurons. 
  </p>
  
  <h3>Spatial Attribution with Saliency Maps</h3>

  <p>
    Attribution techniques are most commonly used to produce <em>saliency maps</em> that associate pixels of the input image to the output classification. While such maps help us identify which pixels the network deemed most important to the ultimate class, it can be difficult to make sense of this information. The meaning of a pixel is extremely entagled with other pixels, and is far-removed from high-level concepts like the output class.
  </p>

  <!-- Cite Grad-CAM work. -->
    
  <p>
    However, if we treat saliency maps as another user interface building block, we can instead apply them to the hidden layers of a neural network.
    Applying attribution to spatial activations of hidden layers changes the question we are asking.
    Rather than asking whether particular pixels being black was important for an image to be classified as "Labrador retriever," we instead ask whether high-level ideas (such as "floppy ear") detected at a position were important instead.
    This approach is similar to what Class Activation Mapping (CAM) methods<d-cite key="zhou2016learning,selvaraju2016grad"></d-cite> do, but they interpret their results as a saliency map on the input image.
  </p> 

  <figure id="AttributionSpatial" class="full-width" style="margin-right: 25px;"></figure>

  <p>
    Layer-to-layer attribution.
    Layers side-by-side.
    Saliency map not just of a single class but multiple outputs.
    Information scent with NMF.

    No one is thinking of these problems as UI problems.
  </p>

  <p>
    By performing attribution in this way, we reason about how the network develops its understanding, and why it does so, using higher-level concepts. For example, how do low-level "fur" detectors combine together to form a higher-level "floppy ear" detector? Was it the "floppy ear," "snout," or "forehead" detectors that increased the model's belief in "labrador retriever" over "beagle"?
  </p>

  <h3>Channel Attribution</h3>

  <p>
    Saliency maps implicitly slice our cube of activations: applying attribution to the spatial positions of a hidden layer aggregates over all channels. As a result, we cannot tell which specific detectors ("floppy ear," "snout," etc.) at each position most contributed to the final "labrador retriever" classification.
  </p>

  <figure id="CubeSlicing"></figure>

  <p>
    An alternate slicing of the cube might instead prioritize channels of spatial locations. Doing so would allow us to perform channel attribution: how much did each detector contribute to the final output? Such an approach is similar to contemporaneous work by Kim et al.
    <d-cite key="kim2017tcav"></d-cite>, who do attribution to learned combination of channels.
  </p>

  <figure class="base-grid" id="AttributionChannel"></figure>

  <p>Of course, these two types attributions need not live in isolation. We can bring both spatial and channel attribution together in a single coherent interface.</p>

  <figure id="SpatialChannelAttribution"></figure>

  <p>
    Attribution to spatial locations and channels can reveal powerful things about a model, especially when we combine them together. Unfortunately, this family of approaches is burdened by two significant problems. On the one hand, it is very easy to end up with an overwhelming amount of information: it would take hours of human auditing
    to understand the long-tail of channels that slightly impact the output. On the other hand, both the aggregations we have explored are extremely lossy and can miss important parts of the story. And, while we could avoid lossy aggregation by working with individual neurons, and not aggregating at all, this explodes the first problem combinatorially.
  </p>

  <!-- <p>
    Attribution is normally used to make <i>saliency maps</i>, visualizations
    describing what part of an input was important in making a decision.
    While saliency maps can be helpful, limiting attribution to them is a
    terrible waste.
    Attribution can be used
    to explore how any neuron effects any other neuron in a network.
    We need only make interfaces that take advantage of this flexibility.
  </p>


  <p>
    For example, consider the PCA of activations from earlier. 
    Attribution can tell us how each principal component tends to effect
    the probability of likely classes, and how each position actually does.
    Note that, unlike a saliency map which only shows how one output is
    effected, this visualization shows how multiple outputs are impacted.
  </p> -->

  <figure class="l-body">
    <img src="images/attribution-multi.png" ></img>
  </figure>
  
  <!-- <p>
    While attribution is usually focused on spatial locations, as above,
    we can just as easily point it at channels.
    Combined with feature visualization, this can allow us to "stand in the
    middle of the network" and observe what is being detected at a hidden layer 
    and how that influences the final outcome.
  </p> -->

  <figure class="l-page">
    <img src="images/attribution-channels.png" ></img>
  </figure>
  
  <p>
    
  </p>

  <h2>Neuron Groups: (TODO, groups or factors)</h2>



  <p>
    In previous sections, we've considered three ways of slicing the cube of activations: into spatial activations, channels, and individual neurons. 
    Each of these has major downsides.
    If one work's with only spatial activations or channels, they miss out on very important parts of the story.
    For example it's interesting that the floppy ear detector helped us classify an image as a Labrador retriever, but it's much more interesting when that's combined with the locations it fired to to do so.
    One can try to drill down to the level of neurons to tell the whole story, but the tens of thousands of neurons are simply too much information.
    Even the hundreds of channels, before being split into individual neurons, can be overwhelming to show users!
  </p>
  
  <p>
    If we want to make useful interfaces into neural networks, it isn't enough to make things meaningful.
    We need to make them human scale, rather than overwhelming dumps of information.
    The key to doing so is finding more meaningful ways of breaking up our activations.
    There's a lot of reason to believe that such decompositions exist.
    Often, many channels or spatial positions will work together in a highly correlated way and are most useful to think of as one unit.
    Other channels or positions will have very little activity, and can be ignore for a high-level overview.
    So, it seems like we ought to be able to find better decompositions if we had the right tools.
  </p>

  <p>
    There is an entire field of research, called matrix factorization, that studies optimal strategies for breaking up matrices. By flattening our cube into a matrix of spatial locations and channels, we can apply these techniques to get more meaningful groups of neurons. These groups will not align as naturally with the cube as the groupings we previously looked at. Instead, they will be combinations of spatial locations and channels. Moreover, these groups are constructed to explain the behavior of a network on a particular image. It would not be effective to reuse the same groupings on another image; each image requires calculating a unique set of groups.
  </p>

  <figure id="NeuronGroupsCube">
    <img src="images/cube-slices.svg"></img>
  </figure>

  <p>
    The groups that come out of this factorization will be the atoms of the interface a user works with. Unfortunately, any grouping is inherently a tradeoff between reducing things to human scale and, because any aggregation is lossy, preserving information. Matrix factorization lets us pick what our groupings are optimized for, giving us a better tradeoff than the natural groupings we saw earlier.
  </p>

  <p>
    (Recent work has explored other techniques for finding meaningful directions in activation space <d-cite key="raghu2017svcca,kim2017tcav"></d-cite>. 
    While this work primarily focuses on finding "globally" meaningful directions, we instead focus on creating smaller numbers of directions to explain individual examples.)
  </p>

  <!-- NIPS Disentagling workshop. -->


  <p>
    The goals of our user interface should influence what we optimize our matrix factorization to prioritize. For example, if we want to prioritize what the network detected, we would want the factorization to fully describe the activations. If we instead wanted to prioritize what would change the network's behaviour, we would want the factorization to fully describe the gradient. Finally, if we want to prioritize what caused the present behaviour, we would want the factorization to fully describe the attributions. Of course, we can strike a balance between these three objectives rather than optimizing one to the exclusion of the others. 
  </p>

  <p>
    Consider the following figure. [Although it looks similar to the diagram we saw previously,] matrix factorization has reduced the overwhelmingly large number of neurons into a small set of groups. This allows us to more concisely distill the story of the neural network.
  </p>

  <figure class="l-page-outset">
    <img src="images/groups.png"></img>
  </figure>

  <p>
    This figure only focuses at a single layer but, as we saw earlier, it can be useful to look across multiple layers to understand how a neural network assembles together lower-level detectors into higher-level concepts. 
  </p>

  <p>
    The groups we constructed before were optimized to understand a single layer independent of the others. To understand multiple layers together, we would like each layer's factorization to be "compatible"&mdash;to have the groups of earlier layers naturally compose into the groups of later layers. This is also something we can optimize the factorization for.
  </p>

  <figure id="AttributionGroups" class="full-width" style="margin-right: 25px;"></figure>

  <p>
    In this section, we recognize that the way in which we break apart the cube of activations is an important interface decision. Rather than resigning ourselves to the natural slices of the cube of activations, we construct more optimal groupings of neurons. These improved groupings are both more meaningful and more human-scale, making it less tedious for users to understand the behaviour of the network.
  </p>

  <!-- <p>
    Future directions: there different goals of activations, gradients, attribution. Composing groups together (sets of groups) that work naturally work (both within the layer, and across layers). Creating groups that you split up "dive into": small groups that capture 80% of the story, and then drill down. 

    Connecting to disentangling. 

    Value in factorizations that are "global." Are there ways to fuse these with the bespoke? Or mix between?
  </p> -->

  <h2>Conclusion &amp; Future Work</h2>

  <p>
    The interface ideas we present in this article are only a few points in a larger design space.

    Many of the design decisions we made with each building block were independent. 
    
    So, at the very least, there are lots of combinations to be explored.

    But, this is just scratching the surface of possibilities: the design space is likely much vaster than these initial examples suggest.
  </p>

  <p>
    As we observed throughout the article, there are many directions each building block could be extended.

    Moreover, we think there are entirely new building blocks whose discovery will expand the space.

    For example, Koh et al. suggest ways of understanding the influence of dataset examples on model behavior<d-cite key="koh2017understanding"></d-cite>&mdash;this functionality is another building block that can be fully composed with the ones we discussed earlier.

    Beyond this, alternate model types (e.g., GANs) may need wholly different building blocks altogether.
  </p>

  <p>
    Another direction would be to more formally study the design space itself.

    Besides enumerating all viable instances within the space, what are the principles that define it?

    The interfaces in this article have different strengths: some emphasize <i>what</i> the network recognizes while others prioritize <i>how</i> its understanding develops, and still others focus on making things <i>human-scale</i>.

    But are these tradeoffs even necessary&mdash;can an interface have its cake and eat it too? 

    If not, what particular sets of tradeoffs are most useful and how do we evaluate them?
  </p>

  <p>
    There is a rich design space for interacting with enumerative algorithms, and we believe an equally rich space exists for interacting with neural networks.

    <!-- The design space for interacting with enumerative algorithms is rich and vast, and we believe the same holds true for the space of interacting with neural networks. -->

    <!-- One important role of these interfaces will be to extend the range of our own thoughts by leaning on the model's understanding of data.

    More importantly, they will be critical for maintaining meaningful human control over increasingly automated processes. -->

    These interfaces will be necessary for meaningful human oversight and critical for endorsing a model's decision-making process.
  </p>
</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>

  <h3>Author Contributions</h3>
  <p>
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
