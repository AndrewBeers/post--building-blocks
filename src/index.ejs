<!doctype html>
<script src="template.v2.js"></script>
<d-front-matter>
  <script type="text/json">{
  "title": "Article Title",
  "description": "Description of the post",
  "authors": [
    {
      "author":"Authors",
      "authorURL": "test",
      "affiliation":"Google Brain",
      "affiliationURL":"https://g.co/brain"
    },

  ]
  }</script>
</d-front-matter>

<!-- <style>
  figure p.heading {
    margin-bottom: 5px;
    font-weight: bold;
    clear: both;
  }

  figure p.heading code {
    font-family: monospace;
    font-size: 80%;
    font-weight: normal;
    margin-left: 5px;
  }

  ul.attribution_list { 
    flex: 1; 
    list-style-type: none;
    padding-left: 0;
  }

  ul.attribution_list li {
    position: relative;
    margin-bottom: 5px;
    padding-left: 5px;
    font-size: 90%;
  }

  ul.attribution_list li.selected { font-weight: bold; }

  ul.attribution_list .scent {
    position: absolute;
    left: 0;
    height: 100%;
    z-index: -1;
  }
</style> -->


<!-- Padding because of /d-article hack for Chrome -->
<d-article style="padding-bottom: 0px;">
  
  <d-title>
    <h1>Putting the Pieces of Interpretability Together</h1>
  </d-title>
  
  <p>
    Over the last couple of years, researchers have developed fundamental building blocks for understanding the inner workings of neural networks.
    We believe that transparency research is now at a point where&mdash;if we bring these building blocks together in the right way&mdash;we can provide meaningful interfaces into how neural networks come to the decisions they do.
  </p>

  <!--
  <p>  
    In particular, significant progress has been made on feature visualization, which gives us examples of what neurons are looking for, and attribution, which tells us how neurons affect the outputs. These two techniques can be very powerful when combined.
  </p>
-->

  
  <h2>Feature Visualization makes Activations Meaningful</h2>



  <p>In computer vision, we use neural networks that run the same detectors at every position in an image. One way we can represent the overall understanding a neural network develops is as a three-dimensional grid. Each position is the amount a "neuron" fired, the x- and y-axes correspond to positions in the image, and the z-axis is the channel, the detector being run. When people talk about neural networks being a black box, they mean that activations are an inscrutable cube of numbers.</p>

  <figure id="CubeDiagram" class="l-body"></figure>
 
 <br>

  <figure id="ExamplePicker" class="l-body" style="margin-bottom: 0px;"></figure>

  <figure id="NeuronVis" class="l-body" style="height: 256px;"></figure>
  
  <figure id="ActivationVis" class="l-body"></figure>

  
  <p>
    By applying this technique to the activation vectors at each position,
    we can see how the network understands the input as a whole
    -- and how that understanding evolves as we move through the layers of the 
    network.
  </p>

</d-article>

  <figure id="AllActivationGrids" style="overflow: hidden; margin-left: 100px;"></figure>

<!-- Padding because of /d-article hack for Chrome -->
<d-article style="padding-bottom: 0px;">
  <p>
    One weakness of this visualization is that it doesn't really show the magnitude of the activations. 
    It shows us what was detected at each position, but not how strongly it was detected.
    To better show magnitudes, let's size the visualizations by the magnitude of the activation vectors:
  </p>
</d-article>

  <figure id="AllActivationGridsMagnitude" style="overflow: hidden; margin-left: 100px;"></figure>

<!-- Padding because of /d-article hack for Chrome -->
<d-article style="padding-bottom: 0px;">
  <!--
  <p>
    We can also get more a less noisy "overview" of the activations vectors for
    an image by computing the first three principal components of the
    activations and coloring the image base on those. These principal components
    are directions in activation space, and having meaning by themselves.
  </p>


  <figure class="l-body">
    <img src="images/activation-pca.png" ></img>
  </figure>

  <p>
    In the end, we have three ways of thinking about an activation vector:
    what it represents as a whole, the principal components, and the individual
    neurons.
  </p>

  <figure class="l-page">
    <img src="images/vector-eq-full.png"  style="width:100%"></img>
  </figure>
  -->

  <!-- <p>These activation grids show us what the network is detecting at each position, but that is only part of the story. To really understand the model, we also want to understand how the relationships between layers affects the output. 
    
    
    To really understand the model, we also want to relationships between layers -- and especially how they affect the output. </p> -->


  <p>Feature visualization shows us _what_ the network detected at each layer. This is useful for... But, it does not describe the consequences these detections. For example, which features in earlier layers caused us to detect a particular class? Or, how does the thing we detected were assembled? And, most importantly, how are the things we detected affect the final output?</p>

  To answer these questions about relationships between neurons, we will need an additional set of techniques.
  </p>


  Feature visualization shows us _what_ the network detected at each layer (for example, "fur,", "floppy ear," and finally "labrador retriever"). However, it does not allow us to reason about _how_ the network assembled these individual pieces to arrive at later decisions, or _why_ these decisions were made. For example, how did "fur" become "floppy ear"? Why did the model decide that "labrador retriever" was more likely than "beagle"? 
  
  These questions are really about the relationship between neurons. To answer these questions, we will need to know more than what a neuron represents. We need an additional set of techniques.


  <!-- Feature visualization allows us to understand what a node represents/neuron is looking for.  -->

  <h2>Attribution shows how Activations Influence the Output</h2>

  <p>Attribution is a set of techniques for explaining the relationships between neurons. Typically, it is simply used to associate the pixels of the input image to the output classification -- known as a <i>saliency map</i>. If you only look at the input and output, you're missing out on all the rich behavior we saw above in the hidden layers.<d-footnote>
    It's worth noting that attribution to hidden units like this may be more natural than trying to attribute all the way back
    to pixels. It isn't entirely clear what attribution to pixels really means. The meaning of a pixel is extremely entangled
    with other pixels, very far removed from high-level ideas like output class. And neural networks need to be invariant to
    things like generic brightening of the image.

    <!-- <figure id="InputSaliencyMapComparedToHigherLevel"></figure> -->

  </d-footnote> Each hidden layer offers us the opportunity to ask different and interesting questions. For example, we know we have a "floppy ear" detector, but why is it firing? And how did the network go from input pixels, to low-level "fur" detection, to this higher-level concept of "floppy ears?"</p>

  <p>Applying attribution to hidden layers does not require inventing new techniques! Almost all our existing attribution techniques can be applied to hidden layers, with only minor modifications. </p>

  <!-- Cite Grad-CAM work. -->

</d-article>
  <figure id="AttributionSpatialHidden" style="overflow: hidden; margin-left: 100px; margin-right: 25px;"></figure>

<d-article>
  <p>By applying attribution to the spatial positions of the hidden layer, our figure above is implicitly slicing the cube of activations described earlier, and aggregating over the channels. Aggregating in this way, however, throws away information about whether it was the "floppy ear," "snout," or other detector **at this position** that caused the final "labrador retriever" classification. </p>

  <p>An alternate slicing of the cube might instead prioritize these channels over spatial locations.</p>

  <figure id="CubeSlicing"></figure>

  <p>One use of this type of slice allows us to perform channel attribution: how much did each detector contribute to the final output?</p>

  <figure id="ChanenlAttrInteractive"></figure>

  <p>And, of course, such attributions need not live in isolation. We can bring both spatial and channel attribution together in a single coherent interface.</p>

  <figure id="SpatialChannelAttribution"></figure>

  <p>
    Attribution to spatial locations and channels can reveal powerful things about a model, especially when we combine them together. Unfortunately, this family of approaches is burdened by two significant problems. On the one hand, it is very easy to end up with an overwhelming amount of information: it would take hours of human auditing
    to understand the long-tail of channels that slightly impact the output. On the other hand, both the aggregations we have explored are extremely lossy and can miss important parts of the story. And, while we could avoid lossy aggregation by working with individual neurons, and not aggregating at all, this explodes the first problem combinatorially.
  </p>

  <!-- <p>
    Attribution is normally used to make <i>saliency maps</i>, visualizations
    describing what part of an input was important in making a decision.
    While saliency maps can be helpful, limiting attribution to them is a
    terrible waste.
    Attribution can be used
    to explore how any neuron effects any other neuron in a network.
    We need only make interfaces that take advantage of this flexibility.
  </p>


  <p>
    For example, consider the PCA of activations from earlier. 
    Attribution can tell us how each principal component tends to effect
    the probability of likely classes, and how each position actually does.
    Note that, unlike a saliency map which only shows how one output is
    effected, this visualization shows how multiple outputs are impacted.
  </p> -->

  <figure id="AttributionSpatial" class="l-body"></figure>

  <figure id="AttributionNeurons" class="l-body"></figure>

  <figure class="l-body">
    <img src="images/attribution-multi.png" ></img>
  </figure>
  
  <!-- <p>
    While attribution is usually focused on spatial locations, as above,
    we can just as easily point it at channels.
    Combined with feature visualization, this can allow us to "stand in the
    middle of the network" and observe what is being detected at a hidden layer 
    and how that influences the final outcome.
  </p> -->

  <figure class="l-page">
    <img src="images/attribution-channels.png" ></img>
  </figure>
  
  <p>
    
  </p>

  <h2>Neuron Groups: (TODO, groups or factors)</h2>

  <p>
    So far, we have considered two very natural ways of slicing the cube of activations. Unfortunately, we have found them to be lacking. Is it possible to do better?
  </p>

  <p>
    There is an entire field of research, called matrix factorization, that studies optimal strategies for breaking up matrices. By flattening our cube into a matrix of spatial locations and channels, we can apply these techniques to get more meaningful groups of neurons. These groups will not align as naturally with the cube as the groupings we previously looked at. Instead, they will be combinations of spatial locations and channels. Moreover, these groups are constructed to explain the behavior of a network on a particular image. It would not be effective to reuse the same groupings on another image; each image requires calculating a unique set of groups.
  </p>

  <figure id="NeuronGroupsCube"></figure>

  <p>
    The groups that come out of this factorization will be the atoms of the interface a user works with. Unfortunately, any grouping is inherently a tradeoff between reducing things to human scale and, because any aggregation is lossy, preserving information. Matrix factorization lets us pick what our groupings are optimized for, giving us a better tradeoff than the natural groupings we saw earlier.
  </p>

  <p>
    The goals of our user interface should influence what we optimize our matrix factorization to prioritize. For example, if we want to prioritize what the network detected, we would want the factorization to fully describe the activations. If we instead wanted to prioritize what would change the network's behaviour, we would want the factorization to fully describe the gradient. Finally, if we want to prioritize what caused the present behaviour, we would want the factorization to fully describe the attributions. Of course, we can strike a balance between these three objectives rather than optimizing one to the exclusion of the others. 
  </p>

  <p>
    Consider the following figure. [Although it looks similar to the diagram we saw previously,] matrix factorization has reduced the overwhelmingly large number of neurons into a small set of groups. This allows us to more concisely distill the story of the neural network.
  </p>

  <figure class="l-page-outset">
    <img src="images/groups.png"></img>
  </figure>

  <p>
    This figure only focuses at a single layer but, as we saw earlier, it can be useful to look across multiple layers to understand how a neural network assembles together lower-level detectors into higher-level concepts. 
  </p>

  <p>
    The groups we constructed before were optimized to understand a single layer independent of the others. To understand multiple layers together, we would like each layer's factorization to be "compatible"&mdash;to have the groups of earlier layers naturally compose into the groups of later layers. This is also something we can optimize the factorization for.
  </p>

  <figure id="Ian"></figure>

  <p>

  </p>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>

  <h3>Author Contributions</h3>
  <p>
  </p>
  <d-bibliography src="bibliography.bib"></d-bibliography>
</d-appendix>
