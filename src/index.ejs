<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>

  <style>
    figure.full-width {
      grid-column: screen;
      margin-left: 100px;
    }

    figure.act-grids {
      overflow: hidden;
    }
  </style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Article Title",
  "description": "Description of the post",
  "authors": [
    {
      "author":"Authors",
      "authorURL": "test",
      "affiliation":"Google Brain",
      "affiliationURL":"https://g.co/brain"
    }
  ]
  }</script>
</d-front-matter>

<d-title>
  <h1>Interfaces for Meaningful Interpretability</h1>
</d-title>

<d-article>
  <p>
    Over the last couple of years, researchers have developed a variety of techniques for understanding the inner workings of neural networks. 
    In this article, we describe how these techniques can serve as fundamental building blocks for new, more <strong>meaningful</strong> interfaces for interpretability.
  </p>

  <p>
    We focus our attention on a network's hidden layers, showing how feature visualization can transform these abstract vector spaces into semantically-meaningful dictionaries. 
    In doing so, we identify three goals for effective user interfaces for interpretability: showing <strong>what</strong> the network detects, explaining <strong>relationships between</strong> parts of the network, and finally making things <strong>human-scale</strong>.
  </p>

  <h2>Making Sense of Hidden Layers</h2>

  <p>
    Existing interpretability techniques are primarily concerned with a neural network's input and output layers. 
    Arguably, this focus is due to the clear meaning these layers have: in computer vision, the input layer represents values for the red, green, and blue color channels for every pixel in the input image, while the output layer consists of class labels and their associated probabilities.
  </p>

  <p>
    However, the power of neural networks lie in their hidden layers&mdash;at every layer, the network discovers a new representation of the input. 
    In computer vision, we use neural networks that run the same feature detectors at every position in the image. 
    We can think of each layer's learned representation as a three-dimensional cube. Each cell in the cube is an <em>activation</em>, or the amount a neuron fires. 
    The x- and y-axes correspond to positions in the image, and the z-axis is the channel (or detector) being run.
  </p>

  <figure id="ActivationCube" class="l-body"></figure>

  <figure id="ExamplePicker" class="l-body" style="margin: 0px;"></figure>

  <figure id="SemanticDict"></figure>

  <p>
    This transformation makes it more straightforward to apply existing interpretability techniques to hidden layers. 
    More importantly, these techniques need no longer live in isolation. 
    Instead, we are now able to treat them as <em>composable</em> building blocks. 
    By bringing them together&mdash;in the right way&mdash;we can provide new user interfaces that shed more meaningful light into how neural networks come to the decisions they do.
  </p>

  <h2>What Does the Network See?</h2>
  
  <figure id="ActivationVecVis" class="l-body" style="margin-top: 0;"></figure>

  <p>
    Applying this technique to all the activation vectors allows us to not only see what the network detects at each position, but also what the network understands of the input image as a whole.
    By using feature visualization in this way, we also move through the layers of the network to observe how the network's understanding evolves: from detecting edges in earlier layers, to more sophisticated shapes and object parts in the latter.
  </p>

  <figure id="AllActivationGrids" class="act-grids full-width"></figure>

  <p>
    These visualizations, however, omit a crucial piece of information: the magnitude of the activations. 
    By sizing each of the cells of the visualization by the magnitude of the activation vector, we can indicate how strongly the network detected features at that position:
  </p>

  <figure id="AllActivationGridsMagnitude" class="act-grids full-width"></figure>

  <p>
    Traditional dimensionality reduction techniques, such as principal component analysis, give us another way to understand what the network is detecting. 
    For instance, to get an "overview" of the activation vectors, we can compute the first three principal components of the activations and color the image based on them. 
  </p>

  <p>
    These principle components are also directions in activation space and, thus, have meaning by themselves. 
    Using feature visualization, we can inspect what each of the components represents.
  </p>

  <figure class="l-body">
    <img src="images/activation-pca.png"></img>
  </figure>

  <p>
    In the end, we have three ways of thinking about an activation vector:
    what it represents as a whole, the principal components, and the individual
    neurons.
  </p>

  <figure class="l-page">
    <img src="images/vector-eq-full.png"  style="width:100%"></img>
  </figure>

  <h2>Attribution: The How &amp; Why</h2>

  <p>
    Feature visualization and dimensionality reduction techniques help answer the first of our three original questions: <em>what</em> does the network detect. However, neither technique allows us to reason about <em>how</em> the network assembled these individual pieces to arrive at later decisions, or <em>why</em> these decisions were made.
  </p>

  <p>
    Attribution is a set of techniques that answers such questions by explaining the relationships between neurons. 
  </p>
  
  <h3>Spatial Attribution with Saliency Maps</h3>

  <p>
    Attribution techniques are most commonly used to produce <em>saliency maps</em> that associate pixels of the input image to the output classification. While such maps help us identify which pixels the network deemed most important to the ultimate class, it can be difficult to make sense of this information. The meaning of a pixel is extremely entagled with other pixels, and is far-removed from high-level concepts like the output class.
  </p>

  <!-- Cite Grad-CAM work. -->
    
  <p>
    However, if we treat saliency maps as another user interface building block, we can instead apply them to the hidden layers of a neural network:
  </p> 

  <figure id="AttributionSpatial" class="full-width" style="margin-right: 25px;"></figure>

  <p>
    By performing attribution in this way, we reason about how the network develops its understanding, and why it does so, using higher-level concepts. For example, how do low-level "fur" detectors combine together to form a higher-level "floppy ear" detector? Was it the "floppy ear," "snout," or "forehead" detectors that increased the model's belief in "labrador retriever" over "beagle"?
  </p>

  <h3>Channel Attribution</h3>

  <p>
    Saliency maps implicitly slice our cube of activations: applying attribution to the spatial positions of a hidden layer aggregates over all channels. As a result, we cannot tell which specific detectors ("floppy ear," "snout," etc.) at each position most contributed to the final "labrador retriever" classification.
  </p>

  <figure id="CubeSlicing"></figure>

  <p>
    An alternate slicing of the cube might instead prioritize channels of spatial locations. Doing so would allow us to perform channel attribution: how much did each detector contribute to the final output?
  </p>

  <figure class="l-page-outset" id="AttributionChannel"></figure>

  <p>Of course, these two types attributions need not live in isolation. We can bring both spatial and channel attribution together in a single coherent interface.</p>

  <figure id="SpatialChannelAttribution"></figure>

  <p>
    Attribution to spatial locations and channels can reveal powerful things about a model, especially when we combine them together. Unfortunately, this family of approaches is burdened by two significant problems. On the one hand, it is very easy to end up with an overwhelming amount of information: it would take hours of human auditing
    to understand the long-tail of channels that slightly impact the output. On the other hand, both the aggregations we have explored are extremely lossy and can miss important parts of the story. And, while we could avoid lossy aggregation by working with individual neurons, and not aggregating at all, this explodes the first problem combinatorially.
  </p>

  <!-- <p>
    Attribution is normally used to make <i>saliency maps</i>, visualizations
    describing what part of an input was important in making a decision.
    While saliency maps can be helpful, limiting attribution to them is a
    terrible waste.
    Attribution can be used
    to explore how any neuron effects any other neuron in a network.
    We need only make interfaces that take advantage of this flexibility.
  </p>


  <p>
    For example, consider the PCA of activations from earlier. 
    Attribution can tell us how each principal component tends to effect
    the probability of likely classes, and how each position actually does.
    Note that, unlike a saliency map which only shows how one output is
    effected, this visualization shows how multiple outputs are impacted.
  </p> -->

  <figure class="l-body">
    <img src="images/attribution-multi.png" ></img>
  </figure>
  
  <!-- <p>
    While attribution is usually focused on spatial locations, as above,
    we can just as easily point it at channels.
    Combined with feature visualization, this can allow us to "stand in the
    middle of the network" and observe what is being detected at a hidden layer 
    and how that influences the final outcome.
  </p> -->

  <figure class="l-page">
    <img src="images/attribution-channels.png" ></img>
  </figure>
  
  <p>
    
  </p>

  <h2>Neuron Groups: (TODO, groups or factors)</h2>

  <p>
    So far, we have considered two very natural ways of slicing the cube of activations. Unfortunately, we have found them to be lacking. Is it possible to do better?
  </p>

  <p>
    There is an entire field of research, called matrix factorization, that studies optimal strategies for breaking up matrices. By flattening our cube into a matrix of spatial locations and channels, we can apply these techniques to get more meaningful groups of neurons. These groups will not align as naturally with the cube as the groupings we previously looked at. Instead, they will be combinations of spatial locations and channels. Moreover, these groups are constructed to explain the behavior of a network on a particular image. It would not be effective to reuse the same groupings on another image; each image requires calculating a unique set of groups.
  </p>

  <figure id="NeuronGroupsCube"></figure>

  <p>
    The groups that come out of this factorization will be the atoms of the interface a user works with. Unfortunately, any grouping is inherently a tradeoff between reducing things to human scale and, because any aggregation is lossy, preserving information. Matrix factorization lets us pick what our groupings are optimized for, giving us a better tradeoff than the natural groupings we saw earlier.
  </p>

  <p>
    The goals of our user interface should influence what we optimize our matrix factorization to prioritize. For example, if we want to prioritize what the network detected, we would want the factorization to fully describe the activations. If we instead wanted to prioritize what would change the network's behaviour, we would want the factorization to fully describe the gradient. Finally, if we want to prioritize what caused the present behaviour, we would want the factorization to fully describe the attributions. Of course, we can strike a balance between these three objectives rather than optimizing one to the exclusion of the others. 
  </p>

  <p>
    Consider the following figure. [Although it looks similar to the diagram we saw previously,] matrix factorization has reduced the overwhelmingly large number of neurons into a small set of groups. This allows us to more concisely distill the story of the neural network.
  </p>

  <figure class="l-page-outset">
    <img src="images/groups.png"></img>
  </figure>

  <p>
    This figure only focuses at a single layer but, as we saw earlier, it can be useful to look across multiple layers to understand how a neural network assembles together lower-level detectors into higher-level concepts. 
  </p>

  <p>
    The groups we constructed before were optimized to understand a single layer independent of the others. To understand multiple layers together, we would like each layer's factorization to be "compatible"&mdash;to have the groups of earlier layers naturally compose into the groups of later layers. This is also something we can optimize the factorization for.
  </p>

  <figure id="Ian"></figure>

  <p>

  </p>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>

  <h3>Author Contributions</h3>
  <p>
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- <d-bibliography src="bibliography.bib"></d-bibliography> -->

</body>